{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Reviews Text Analysis Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import math\n",
    "import csv\n",
    "import time\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "#sqlite_file = \"C:\\\\Users\\\\kevin\\\\Desktop\\\\CS 175\\\\Project\\\\sqlite\\\\my_db.sqlite\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "#conn = sqlite3.connect(sqlite_file)\n",
    "#c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets analyze the spread of the number of reviews across all movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_rows = c.execute(\"SELECT imdb_id FROM reviews\").fetchall()\n",
    "\n",
    "review_counts = {}\n",
    "count = 0\n",
    "for review in review_rows:\n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        imdb_id = review[0]\n",
    "        if imdb_id in review_counts:\n",
    "            review_counts[imdb_id] += 1\n",
    "        else:\n",
    "            review_counts[imdb_id] = 1\n",
    "        if (count % 100 == 0):\n",
    "            print(count)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selecting all movies that have at least 1 review in the first three weeks\n",
    "\n",
    "review_rows = c.execute(\"SELECT imdb_id FROM reviews\").fetchall()\n",
    "\n",
    "imdb_ids = []\n",
    "reviews = []\n",
    "three_weeks_count = []\n",
    "\n",
    "for review in review_rows:\n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        imdb_id = review[0]\n",
    "        if imdb_id in review_counts:\n",
    "            review_counts[imdb_id] += 1\n",
    "        else:\n",
    "            review_counts[imdb_id] = 1\n",
    "        if (count % 100 == 0):\n",
    "            print(count)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(review_totals, normalized_boxoffice, test_size=.25)\n",
    "\n",
    "x_train = np.reshape(x_train, (-1,1))\n",
    "x_test = np.reshape(x_test, (-1,1))\n",
    "linear_regression(x_train, x_test, y_train, y_test)\n",
    "random_forests(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of movies counted: \" + str(len(review_counts)))\n",
    "\n",
    "\n",
    "review_totals = []\n",
    "above_40 = 0\n",
    "above_30 = 0\n",
    "above_150 = 0\n",
    "above_20 = []\n",
    "for imdb_id, review_total in review_counts.items():\n",
    "    review_totals.append(review_total)\n",
    "    if review_total > 150:\n",
    "        above_150 += 1\n",
    "    if review_total > 40:\n",
    "        above_40 += 1\n",
    "    if review_total > 30:\n",
    "        above_30 += 1\n",
    "    if review_total > 20:\n",
    "        above_20.append(imdb_id)\n",
    "\n",
    "review_totals = numpy.array(sorted(review_totals))\n",
    "print(\"Mean of values: \" + str(numpy.mean(review_totals)))\n",
    "print(\"Median of values: \" + str(numpy.median(review_totals)))\n",
    "print(\"STD of values: \" + str(numpy.std(review_totals)))\n",
    "print(\"Minimum of values: \" + str(min(review_totals)))\n",
    "print(\"Max of values: \" + str(max(review_totals)))\n",
    "\n",
    "print(\"Number of movies w/ more than 150 reviews: \" + str(above_150))\n",
    "print(\"Number of movies w/ more than 40 reviews: \" + str(above_40))\n",
    "print(\"Number of movies w/ more than 30 reviews: \" + str(above_30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we'll collect all the reviews into datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stmt = \"SELECT imdb_id, len, reviews FROM reviews_three\"\n",
    "response = c.execute(stmt).fetchall()\n",
    "\n",
    "reviews = []\n",
    "box_offices = []\n",
    "imdb_ids = []\n",
    "for row in response:\n",
    "    if row[1] > 0:\n",
    "        imdb_ids.append(row[0])\n",
    "        reviews.append(row[2])\n",
    "        box = c.execute(\"SELECT box FROM movies WHERE imdb_id = '\" + row[0] + \"'\").fetchall()[0][0]\n",
    "        box_offices.append(log(float(box))/log(10))\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "box_offices = lab_enc.fit_transform(box_offices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking all the movies, only 3999 out of the about 5000 have reviews within the first three weeks. Next, like put our data in to the correct format for our linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_split_features(train_data, test_data):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_data, test_data, test_size=.25)\n",
    "\n",
    "    count_vect = CountVectorizer(stop_words = stopWords, min_df = 3)\n",
    "    X_train_counts = count_vect.fit_transform(x_train)\n",
    "    X_test_counts = count_vect.transform(x_test)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    \n",
    "    features = count_vect.get_feature_names()\n",
    "    \n",
    "    print(top_mean_feats(X_train_tfidf, features, top_n = 50))\n",
    "\n",
    "\n",
    "    return (X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top tfidf values in a row and return them with their corresponding feature names\n",
    "# Source: https://buhrmann.github.io/tfidf-analysis.html\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    \n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression(X_train, X_test, Y_train, Y_test):\n",
    "    \n",
    "    from sklearn import linear_model\n",
    "    clf = linear_model.LinearRegression().fit(X_train, Y_train)\n",
    "    predicted_LR = clf.predict(X_test)\n",
    "    \n",
    "    mse = np.mean((predicted_LR - Y_test)**2)\n",
    "    variance = clf.score(X_test, Y_test)\n",
    "    \n",
    "    print(\"Linear Regression: \")\n",
    "    print(\"MSE= \" + str(mse))\n",
    "    print(\"variance= \" + str(variance))\n",
    "    \n",
    "    return predicted_LR\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forests(X_train, X_test, Y_train, Y_test):\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    clf = RandomForestRegressor(n_estimators = 10, max_depth = 4, random_state = 0).fit(X_train, Y_train)\n",
    "    \n",
    "    predicted_RF = clf.predict(X_test)\n",
    "    mse = np.mean((predicted_RF - Y_test)**2)\n",
    "    variance = clf.score(X_test, Y_test)\n",
    "    \n",
    "    print(\"Random Forests: \")\n",
    "    print(\"MSE= \" + str(mse))\n",
    "    print(\"variance= \" + str(variance))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def support_vector(X_train, X_test, Y_train, Y_test):\n",
    "    \n",
    "    from sklearn.svm import SVR\n",
    "    \n",
    "    clf = SVR(C=1.0, epsilon=0.2).fit(X_train, Y_train) \n",
    "    \n",
    "    predicted_SVR = clf.predict(X_test)\n",
    "    mse = np.mean((predicted_SVR - Y_test)**2)\n",
    "    variance = clf.score(X_test, Y_test)\n",
    "    \n",
    "    print(\"Support Vector Regression: \")\n",
    "    print(\"MSE= \" + str(mse))\n",
    "    print(\"variance= \" + str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sqlite3 as sql\n",
    "\n",
    "conn = sql.connect(\"C:\\\\Users\\\\Justin\\\\CS175\\\\movie_meta.db\")\n",
    "conn2 = sql.connect(\"C:\\\\Users\\\\Justin\\\\CS175\\\\comment_set1.db\")\n",
    "\n",
    "result_dict = defaultdict(list)\n",
    "\n",
    "c = conn.cursor()\n",
    "c2 = conn2.cursor()\n",
    "\n",
    "c2.execute(\"SELECT DISTINCT imdb_id FROM comments\")\n",
    "unique = c2.fetchall()\n",
    "unique_list = [x[0] for x in unique]\n",
    "\n",
    "for x in unique_list:\n",
    "        if x == 'tconst' : continue\n",
    "        c.execute(\"SELECT box_office FROM meta WHERE imdb_id = ?\",(x,))\n",
    "        box_office = c.fetchall()[0][0]\n",
    "        result_dict[x].append(math.log(box_office,10))\n",
    "        result_dict[x].append(\"\")\n",
    "\n",
    "c2.execute(\"SELECT imdb_id, body FROM comments\")\n",
    "results = c2.fetchall()\n",
    "\n",
    "for imdb_id, body in results:\n",
    "\n",
    "        try:\n",
    "                if imdb_id == 'tconst': continue\n",
    "                result_dict[imdb_id][1] += body\n",
    "\n",
    "        except Exception as e:\n",
    "                continue\n",
    "\n",
    "reviews = []\n",
    "box_offices = []\n",
    "for x, y in result_dict.items():\n",
    "        box_offices.append(y[0])\n",
    "        reviews.append(y[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature     tfidf\n",
      "0          movie  0.035666\n",
      "1           like  0.020121\n",
      "2           film  0.015711\n",
      "3            one  0.010292\n",
      "4         movies  0.008378\n",
      "5         people  0.007283\n",
      "6          would  0.007203\n",
      "7           http  0.006654\n",
      "8           good  0.006637\n",
      "9            com  0.006162\n",
      "10         think  0.005847\n",
      "11        really  0.005459\n",
      "12           man  0.005300\n",
      "13           www  0.005227\n",
      "14         watch  0.004727\n",
      "15          imdb  0.004304\n",
      "16          best  0.004262\n",
      "17          time  0.003818\n",
      "18          2011  0.003301\n",
      "19         great  0.003265\n",
      "20         title  0.003222\n",
      "21         black  0.003185\n",
      "22         films  0.003165\n",
      "23        batman  0.003131\n",
      "24        horror  0.003108\n",
      "25           see  0.003074\n",
      "26         scene  0.002996\n",
      "27          year  0.002928\n",
      "28            10  0.002928\n",
      "29          bond  0.002926\n",
      "30          also  0.002824\n",
      "31          dead  0.002793\n",
      "32        pretty  0.002790\n",
      "33         story  0.002776\n",
      "34           war  0.002766\n",
      "35          last  0.002710\n",
      "36           bad  0.002690\n",
      "37      superman  0.002665\n",
      "38          seen  0.002640\n",
      "39          book  0.002590\n",
      "40          iron  0.002544\n",
      "41       youtube  0.002512\n",
      "42         night  0.002505\n",
      "43           get  0.002494\n",
      "44  canistreamit  0.002423\n",
      "45    canistream  0.002422\n",
      "46          girl  0.002397\n",
      "47           day  0.002387\n",
      "48       watched  0.002371\n",
      "49          2015  0.002313\n"
     ]
    }
   ],
   "source": [
    "X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf, Y_train, Y_test = extract_split_features(reviews, box_offices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let train our learner from just using the count vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: \n",
      "MSE= 690.033898349\n",
      "variance= -340.611352488\n",
      "Random Forests: \n",
      "MSE= 2.00917464463\n",
      "variance= 0.00532876807907\n"
     ]
    }
   ],
   "source": [
    "predicted_LR = linear_regression(X_train_counts, X_test_counts, Y_train, Y_test)\n",
    "clf_RF = random_forests(X_train_counts, X_test_counts, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: \n",
      "MSE= 3.16276687971\n",
      "variance= -0.565773904687\n"
     ]
    }
   ],
   "source": [
    "predicted_LR = linear_regression(X_train_tfidf, X_test_tfidf, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests: \n",
      "MSE= 1.96846840742\n",
      "variance= 0.0254809849233\n"
     ]
    }
   ],
   "source": [
    "clf_RF = random_forests(X_train_tfidf, X_test_tfidf, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed in our analysis that review sentiment has very little effect on box office. Lets try to remove polarizing words, to test if that is just causing additional noise. We will add in positive and negative words we found in an online corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polarized = []\n",
    "fp = open('positive-words.txt')\n",
    "\n",
    "for i, line in enumerate(fp):\n",
    "    if i > 34:\n",
    "        polarized.append(line.strip())\n",
    "\n",
    "\n",
    "fp = open('negative-words.txt')\n",
    "\n",
    "for i, line in enumerate(fp):\n",
    "    if i > 34:\n",
    "        polarized.append(line.strip())\n",
    "\n",
    "fp.close()\n",
    "new_stopwords = text.ENGLISH_STOP_WORDS.union(polarized)\n",
    "\n",
    "\n",
    "def extract_split_features_no_polarized(train_data, test_data):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_data, test_data, test_size=.25)\n",
    "\n",
    "    count_vect = CountVectorizer(stop_words = new_stopwords, min_df = 3)\n",
    "    X_train_counts = count_vect.fit_transform(x_train)\n",
    "    X_test_counts = count_vect.transform(x_test)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "    return (X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts, X_train_tfidf, X_test_counts, X_test_tfidf, Y_train, Y_test = extract_split_features_no_polarized(reviews, box_offices)\n",
    "predicted_LR = linear_regression(X_train_tfidf, X_test_tfidf, Y_train, Y_test)\n",
    "clf_RF = random_forests(X_train_tfidf, X_test_tfidf, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "review_totals = []\n",
    "normalized_budgets = []\n",
    "normalized_boxoffice = []\n",
    "reviews = []\n",
    "\n",
    "\n",
    "with open('movie_metadata.csv', 'r', encoding='utf-8') as moviecsv:\n",
    "    moviecsv = csv.DictReader(moviecsv)\n",
    "    count = 0\n",
    "    for line in moviecsv:\n",
    "        budget = line[\"Budget\"]\n",
    "        imdb_id = line['tconst']\n",
    "        movie_reviews = c.execute(\"SELECT len, reviews FROM reviews_three WHERE imdb_id ='\" + imdb_id + \"'\").fetchall()\n",
    "        length = movie_reviews[0][0]\n",
    "        da_reviews = movie_reviews[0][1]\n",
    "        if budget != \"0\" and  length > 0 and budget != \"LOOKINTO\":\n",
    "            count += 1\n",
    "            \n",
    "            review_totals.append(length)\n",
    "            reviews.append(da_reviews)\n",
    "            normalized_budgets.append(math.log(int(budget), 10))\n",
    "            \n",
    "            box_office = line['BoxOffice'].split(\".\")[0]\n",
    "            normalized_boxoffice.append(math.log(int(box_office), 10))\n",
    "            \n",
    "            print(count)\n",
    "            print(imdb_id)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(review_totals, normalized_boxoffice, test_size=.25)\n",
    "\n",
    "\n",
    "predicted_LR_lengths = linear_regression(np.reshape(x_train, (-1,1)) , np.reshape(x_test, (-1,1)), y_train, y_test)\n",
    "predicted_RF_lengths = random_forests(np.reshape(x_train, (-1,1)) , np.reshape(x_test, (-1,1)), y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "x_budget_train, x_budget_test, x_total_train, x_total_test, y_bo_train, y_bo_test = train_test_split(normalized_budgets, review_totals, normalized_boxoffice, test_size=.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(len(x_train_tfidf))\n",
    "print(len(x_budget_train))\n",
    "\n",
    "\n",
    "train_data = []\n",
    "feature_names = [\"Budgets\", \"Review Totals\"]\n",
    "for i in range(len(x_budget_train)):\n",
    "    train_data.append([x_budget_train[i], x_total_train[i]])\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(x_budget_test)):\n",
    "    test_data.append([x_budget_test[i], x_total_test[i]])\n",
    "df_train = pd.DataFrame(train_data, columns = feature_names)\n",
    "df_test = pd.DataFrame(test_data, columns = feature_names)\n",
    "target_train = pd.DataFrame(y_bo_train, columns = [\"Box Office\"])\n",
    "target_test = pd.DataFrame(y_bo_test, columns=[\"Box Office\"])\n",
    "\n",
    "\n",
    "X_train = df_train[feature_names]\n",
    "X_test = df_test[feature_names]\n",
    "y_train = target_train[\"Box Office\"]\n",
    "y_test = target_test[\"Box Office\"]\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "predicted_LR = clf.predict(X_test)\n",
    "\n",
    "mse = np.mean((predicted_LR - y_test)**2)\n",
    "variance = r2_score(y_test, predicted_LR)\n",
    "\n",
    "print(\"Linear Regression: \")\n",
    "print(\"MSE= \" + str(mse))\n",
    "print(\"variance= \" + str(variance))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor(n_estimators = 20, max_depth = 7, random_state = 0).fit(X_train, y_train)\n",
    "predicted_RF = clf.predict(X_test)\n",
    "\n",
    "# clf = sm.OLS(y_train, X_train).fit()\n",
    "# predicted_OLS = clf.predict(X_test)\n",
    "\n",
    "mse = np.mean((predicted_RF - y_test)**2)\n",
    "variance = r2_score(y_test, predicted_RF)\n",
    "\n",
    "print(\"Random Forest: \")\n",
    "print(\"MSE= \" + str(mse))\n",
    "print(\"variance= \" + str(variance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[0:5])\n",
    "print(predicted_LR[0:5])\n",
    "print(predicted_RF[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_total_train, y_bo_train, color=\"blue\", )\n",
    "plt.plot(np.unique(x_total_test), \n",
    "         np.poly1d(np.polyfit(x_total_test, y_bo_test, 1))(np.unique(x_total_test)), color='red')\n",
    "\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title(\"Linear Regression for Review Counts & Box Office Revenue\")\n",
    "plt.xlabel(\"Review Counts\")\n",
    "plt.ylabel(\"Box Office Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
